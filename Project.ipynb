{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4KSDFaG28y7g"
   },
   "outputs": [],
   "source": [
    "#THE FOLLOWING CODE IS A COMBINATION OF DIFFERENT FILES \n",
    "#FROM AUTHORS ABDUL, MURTUZA, DEVESH\n",
    "#THE CODE HAS BEEN MOTIVATED FROM DIFFERENT LINKS PROVIDED IN THE REPORT\n",
    "\n",
    "#FILTERING OF THE DATASET\n",
    "#Importing the libraries\n",
    "import os\n",
    "import json\n",
    "import shutil # for copying the files\n",
    "import os.path # for setting the destination folder\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import os, shutil, glob, os.path\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# importing the library for extracting the winrar files\n",
    "try:\n",
    "    import lzma\n",
    "except ImportError:\n",
    "    from backports import lzma\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "# getting current directory\n",
    "print(os.getcwd())\n",
    "\n",
    "# main folder having all hash tags folders\n",
    "main_folder = \"D:\\\\Instagram Data\"\n",
    "\n",
    "# changing cueent directory to main directory\n",
    "os.chdir(main_folder)\n",
    "\n",
    "# writing the folder names in the main folder\n",
    "folders = ['#health', '#medicine']\n",
    "\n",
    "# iterating each folder in the main folder\n",
    "for folder in folders:\n",
    "    # making full apth for the sub folder\n",
    "    full_path = main_folder+\"\\\\\"+folder\n",
    "    # changing the working directory to the subfolder to work on a particulat hash tag folder\n",
    "    os.chdir(full_path)\n",
    "    print(os.getcwd())\n",
    "    folder_contents_names = [name for name in os.listdir()]\n",
    "    #print(\"total files in \", full_path,  \" : \" ,  len(folder_contents_names))\n",
    "    \n",
    "    # destination folder\n",
    "    destination_path = \"D:\\\\insta filter\" \n",
    "    \n",
    "    #making new folder in the destination folder for a hashtag\n",
    "    destination_path = destination_path + \"\\\\\" + folder\n",
    "    \n",
    "    # making new hashtag folder for filtered data\n",
    "    try:\n",
    "        os.mkdir(destination_path)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % destination_path)\n",
    "    \n",
    "    # creating  three lists for winrar, text file and image string names in the hashtag folder\n",
    "    folder_onlywinrarstr=[]\n",
    "    folder_onlytxtstr=[]\n",
    "    folder_onlyjpgsstr=[]\n",
    "    for name in folder_contents_names:\n",
    "        if name.endswith('.xz'):\n",
    "            folder_onlywinrarstr.append(name)\n",
    "        if name.endswith('.txt'):\n",
    "            folder_onlytxtstr.append(name)\n",
    "        if name.endswith('.jpg'):\n",
    "            folder_onlyjpgsstr.append(name)\n",
    "\n",
    "\n",
    "    # counter to check progress\n",
    "    counter = 1\n",
    "    # total winrar files in the hashtag folder\n",
    "    total_files_to_process= len(folder_onlywinrarstr)\n",
    "    \n",
    "    # iterarating all the winrar files in the hasg=htag foolder\n",
    "    for single_winrar_file in folder_onlywinrarstr:\n",
    "\n",
    "        print(\"Working on {} folder. File {} of {}\".format(folder, counter ,total_files_to_process))\n",
    "        counter+=1\n",
    "        \n",
    "        file_name=full_path+\"\\\\\"+single_winrar_file\n",
    "        # opening a winrar json file, reading it in bytes and decoding it from bytes to string in utf 8\n",
    "        # file_name something like D:\\insta data\\#fitnessgoals\\2013-01-04_03-15-37_UTC.json.xz\n",
    "        json_in_str= lzma.open(file_name).read().decode(\"utf-8\")\n",
    "        \n",
    "        json_dict = json.loads(json_in_str)  # converting json from string into dictionary\n",
    "        try:    \n",
    "            address_data= json_dict['node']['location']['address_json'] # going to address key in dictionary\n",
    "            address_data= address_data.replace('false','False') # changing false string in json to False datatype of python\n",
    "            address_data= address_data.replace('true','True') # changing true string in json to True datatype of python \n",
    "            # address_data is string now. eval() will convert string to appropriate python datatype. Thats why false is made False and true is made True\n",
    "            country_code = eval(address_data)['country_code']\n",
    "            \n",
    "            #print(country_code)\n",
    "            #print(single_winrar_file)\n",
    "            \n",
    "            # if country code is empty make folder with name unknown\n",
    "            if len(country_code) == 0:\n",
    "                country_code = 'unknown'\n",
    "            \n",
    "            # taking just the winrar file name (without extension) if it has location data\n",
    "            just_file_name = single_winrar_file[:-8]\n",
    "            \n",
    "            # path to folder that is to be created\n",
    "            folder_to_create= destination_path + \"\\\\\"+ country_code\n",
    "            \n",
    "            #os.path.join(\"c:\", os.sep, \"Users\", \"Mainuser\", \"Desktop\", \"Lab6\")\n",
    "            \n",
    "            # if country folder is not present, create a new folder for a new country\n",
    "            if not os.path.isdir(folder_to_create):\n",
    "                try:\n",
    "                    os.mkdir(folder_to_create) # creating a new country folder\n",
    "                    # there are multiple text files for a single name. few are old and few are new. saving both\n",
    "                    for textfile in folder_onlytxtstr:\n",
    "                        if just_file_name in textfile:\n",
    "                            shutil.copy(textfile, folder_to_create)\n",
    "                    # there are multiple images in a single instagram post. saving all the images of the post\n",
    "                    for imagefile in folder_onlyjpgsstr:\n",
    "                        if just_file_name in imagefile:\n",
    "                            shutil.copy(imagefile, folder_to_create)\n",
    "                except OSError:\n",
    "                    print (\"Creation of the directory %s failed\" % folder_to_create)\n",
    "            # if country folder is already present then save text file and image file to country specific folder\n",
    "            else:\n",
    "                # there are multiple text files for a single name. few are old and few are new. saving both\n",
    "                for textfile in folder_onlytxtstr:\n",
    "                    if just_file_name in textfile:\n",
    "                        shutil.copy(textfile, folder_to_create)\n",
    "                # there are multiple images in a single instagram post. saving all the images of the post\n",
    "                for imagefile in folder_onlyjpgsstr:\n",
    "                    if just_file_name in imagefile:\n",
    "                        shutil.copy(imagefile, folder_to_create)        \n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePc_tkZm9qgA"
   },
   "outputs": [],
   "source": [
    "#Feature extraction VGG16 and KMeans clustering using Imagenet data\n",
    "\n",
    "image.LOAD_TRUNCATED_IMAGES = True\n",
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "# Main Dir after filter\n",
    "maindir = 'D:/Insta filter/#healthgoals/'\n",
    "folder =[]\n",
    "os.chdir(maindir)\n",
    "#Determining the country code in the folder\n",
    "for name in os.listdir(\".\"):\n",
    "    if os.path.isdir(name):\n",
    "        folder.append(name)\n",
    "\n",
    "#Initialize no. of clusters to zero\n",
    "number_clusters = 0\n",
    "onlyfiles = []\n",
    "#Determining the files in the country\n",
    "for name in folder:\n",
    "    ogdir = maindir + name +'/'\n",
    "    os.chdir(ogdir)\n",
    "    #changing the directory and finding jpg files\n",
    "    for file in glob.glob(\"*.jpg\"):\n",
    "        onlyfiles.append(file)\n",
    "    if len(onlyfiles) < 50:\n",
    "        onlyfiles=[]\n",
    "        continue #If the images are less than 50 we do not consider that folder\n",
    "    elif (len(onlyfiles) > 50) and (len(onlyfiles) < 1000):\n",
    "        number_clusters = 5 #For images between 50 and 1000 count we use no of clusters as 5\n",
    "    else:\n",
    "        number_clusters = 10 #For images more than 1000 no. of clusters is 10\n",
    "    if number_clusters != 0:\n",
    "        filelist = glob.glob(os.path.join(ogdir, '*.jpg'))\n",
    "        filelist.sort() #files to extract features\n",
    "        featurelist = []\n",
    "        for i, path in enumerate(filelist):\n",
    "            print(\"    Status: %s / %s\" %(i, len(filelist)), end=\"\\r\")\n",
    "            img = image.load_img(path, target_size=(224, 224)) #loading image 224x224 size\n",
    "            data = image.img_to_array(img) \n",
    "            data = np.expand_dims(data, axis=0)\n",
    "            data = preprocess_input(data)\n",
    "            features = np.array(model.predict(data)) #Features extracted through transformation to array and dimension expansion\n",
    "            featurelist.append(features.flatten()) #storing all features of the images\n",
    "        kmeans = KMeans(n_clusters=number_clusters, random_state=0).fit(np.array(featurelist)) #Performing KMeans clustering on the features\n",
    "        print(\"\\n\")\n",
    "        targetdir = ogdir+\"vgg16/\" #Target Directory images being stored\n",
    "        try:\n",
    "            os.makedirs(targetdir)\n",
    "        except OSError:\n",
    "            pass\n",
    "        for i, m in enumerate(kmeans.labels_):\n",
    "            print(\"    Copy: %s / %s\" %(i, len(kmeans.labels_)), end=\"\\r\")\n",
    "            shutil.copy(filelist[i], targetdir + str(m) + \"_\" + str(onlyfiles[i])) #Copying the images to respective folders\n",
    "\n",
    "        ac = AgglomerativeClustering(n_clusters=number_clusters, affinity='euclidean', linkage='ward').fit(np.array(featurelist))\n",
    "        print(\"\\n\") #Performing Agglomerative Heirarichal Clustering using euclidean distance as the affinity\n",
    "        targetdir = ogdir+\"Agglo/\"\n",
    "        try:\n",
    "            os.makedirs(targetdir)\n",
    "        except OSError:\n",
    "            pass\n",
    "        for i, m in enumerate(ac.labels_):\n",
    "            print(\"    Copy: %s / %s\" %(i, len(ac.labels_)), end=\"\\r\")\n",
    "            shutil.copy(filelist[i], targetdir + str(m) + \"_\" + str(onlyfiles[i])) #Copying images to respective folders\n",
    "        onlyfiles=[] #Initializing no. of files to zero as swithing to new fodler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JO-4x_yU-P0W"
   },
   "outputs": [],
   "source": [
    "#Extracting the top hashtags\n",
    "#top_hash_tags.py\n",
    "\n",
    "# setting main directoty where we have clustered countries\n",
    "main_directory = 'D:/Insta filter/#healthgoals/'\n",
    "\n",
    "os.chdir(main_directory)\n",
    "\n",
    "#getting names of files in main folder\n",
    "country_folders = [name for name in os.listdir()]\n",
    "\n",
    "# dictionary to take all hashtags count\n",
    "over_all_tag_count={}\n",
    "\n",
    "\n",
    "hashtag= \"#healthgoals\"\n",
    "\n",
    "print(\"Top 5 hashtags related to {}\".format(hashtag))\n",
    "\n",
    "# iterating each country\n",
    "for country_name in country_folders:\n",
    "    if os.path.isdir(country_name) and country_name != 'unknown':\n",
    "        each_country_directory = main_directory + '//' + country_name\n",
    "        # getting into each country folder\n",
    "        os.chdir(each_country_directory)\n",
    "        inside_country = os.getcwd()\n",
    "        # selecting the clustered images folder\n",
    "        vgg_folder_path = inside_country + '//' + 'vgg16' \n",
    "        if os.path.exists(vgg_folder_path):\n",
    "            os.chdir(vgg_folder_path)\n",
    "            print()\n",
    "            # getting names of images\n",
    "            clustered_images_names = [name for name in os.listdir()]\n",
    "            \n",
    "            # initial images cluster number is 0\n",
    "            cluster_str_to_save = '0'\n",
    "            # taking a variable for country\n",
    "            country_str= country_name\n",
    "            # dictionary to count hashtags counts for each country\n",
    "            country_tag_count ={}\n",
    "            \n",
    "            os.chdir(inside_country)\n",
    "            \n",
    "            # iterating each image in each country\n",
    "            for each_image in clustered_images_names:\n",
    "                try:\n",
    "                    # getting name of the image\n",
    "                    just_image_name= each_image[2:25]\n",
    "                    txt_file_name =  just_image_name + '.txt'\n",
    "                    # checking for respective txt file with same image name\n",
    "                    txt_content = open(txt_file_name,encoding=\"utf8\") \n",
    "                    # reading the txt file\n",
    "                    txt_content = txt_content.read()\n",
    "                    # getting each word in the text file\n",
    "                    words_in_txt = txt_content.split()\n",
    "                    for word in words_in_txt:\n",
    "                        # checking for # tags in the text files\n",
    "                        if word.startswith(\"#\"):\n",
    "                            if word in country_tag_count:\n",
    "                                country_tag_count[word] = country_tag_count[word]+1\n",
    "                            else:\n",
    "                                country_tag_count[word] = 1\n",
    "                            if word in over_all_tag_count:\n",
    "                                over_all_tag_count[word] = over_all_tag_count[word]+1\n",
    "                            else:\n",
    "                                over_all_tag_count[word] = 1\n",
    "            \n",
    "                    # getting the cluster number\n",
    "                    cluster_str = each_image[:1]\n",
    "                    if cluster_str_to_save != cluster_str:\n",
    "                        country_tag_count_as_list = [country_tag_count]\n",
    "                        \n",
    "                        print(country_name)\n",
    "                        print(cluster_str_to_save)\n",
    "                        \n",
    "                        # sort using a lambda expression to get top hash tags\n",
    "                        sorted_names = sorted(country_tag_count, key=lambda x: country_tag_count[x])\n",
    "\n",
    "                        # getting top 5 hash tags except the hashtag we are using to search for\n",
    "                        val=0\n",
    "                        for k in reversed(sorted_names):\n",
    "                            if k != hashtag:\n",
    "                                print(\"{} : {}\".format(k, country_tag_count[k]))\n",
    "    \n",
    "                                val+=1\n",
    "                                if val==5:\n",
    "                                    break\n",
    "                            \n",
    "                        # getting the cluster number\n",
    "                        cluster_str_to_save = cluster_str\n",
    "                        # making dictionary empty to as to read for next country\n",
    "                        country_tag_count={}\n",
    "                        \n",
    "                        \n",
    "                        print()\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    os.chdir(main_directory)\n",
    "\n",
    "# doing the dsame for all the text files irrespective of countries\n",
    "print(\"For All Countries\")\n",
    "sorted_names = sorted(over_all_tag_count, key=lambda x: over_all_tag_count[x])\n",
    "\n",
    "\n",
    "val=0\n",
    "for k in reversed(sorted_names):\n",
    "    if k != hashtag:\n",
    "        print(\"{} : {}\".format(k, over_all_tag_count[k]))\n",
    "                \n",
    "        val+=1\n",
    "        if val==5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PsNEk5rr-fUl"
   },
   "outputs": [],
   "source": [
    "#NEW FILE\n",
    "# this is used to save the top hash tags in a text files by running the previous file and storing the output in a text file\n",
    "import subprocess\n",
    "with open(\"output.txt\", \"w+\") as output:\n",
    "    subprocess.call([\"python\", \"top_hash_tags.py\"], stdout=output)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
